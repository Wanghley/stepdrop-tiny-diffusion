\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[final]{stepdrop}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math environments
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % include graphics


\title{StepDrop: Accelerating Tiny Diffusion Models with Stochastic Step Skipping}


\author{%
  Wanghley Soares Martins \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
  \texttt{wanghley.martins@duke.edu} \\
  \And
  Nicolas Vasilescu \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
  \And
  Logan Chu \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
}


\begin{document}


\maketitle

\begin{abstract}
Diffusion models achieve state-of-the-art image synthesis but suffer from high inference latency due to their sequential, iterative denoising process—a critical barrier for deployment on resource-constrained edge devices. We observe that diffusion steps contribute unevenly to generation quality: early steps establish coarse structure, late steps refine texture, while middle steps provide minimal perceptual value. Based on this insight, we propose \textbf{StepDrop}, a training-free, plug-and-play acceleration method that stochastically skips low-importance denoising steps. StepDrop employs importance-weighted timestep selection strategies—including uniform, quadratic, cosine, and adaptive schedulers—to concentrate computation on high-value steps while probabilistically omitting redundant mid-trajectory updates. Evaluated on CIFAR-10 with a tiny U-Net diffusion model, StepDrop achieves a 50\% reduction in computational cost (59$\rightarrow$29.5 GFLOPs) while \emph{improving} FID by 2.5 points over DDIM at equivalent NFE budgets. At 50 function evaluations, StepDrop attains FID 76.05 compared to DDIM's 77.20; at 25 NFE, it matches DDIM-50 quality while doubling throughput. Residual analysis confirms strong agreement with full DDIM trajectories, demonstrating robustness to mid-phase skipping. Our results establish that focusing computation on temporally important steps yields superior quality-per-FLOP, enabling practical diffusion model deployment with flexible compute-memory tradeoffs. Code is available at \url{https://github.com/wanghley/stepdrop-tiny-diffusion}.
\end{abstract}

%=============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Generative modeling has witnessed remarkable progress over the past decade, with diffusion models emerging as a dominant paradigm for high-fidelity image synthesis. Unlike Generative Adversarial Networks (GANs), which rely on adversarial training dynamics prone to mode collapse, or Variational Autoencoders (VAEs), which often produce blurry outputs, diffusion models achieve state-of-the-art generation quality through a principled probabilistic framework based on iterative denoising~\citep{ho2020denoising, song2020denoising}.

The core mechanism of diffusion models involves two processes: a \emph{forward process} that gradually corrupts data by adding Gaussian noise over many timesteps, and a \emph{reverse process} that learns to denoise, progressively recovering structure from pure noise. This formulation enables stable training and produces samples of exceptional quality, powering applications from text-to-image generation to drug discovery and audio synthesis.

However, the iterative nature of diffusion sampling comes at a significant computational cost. The standard Denoising Diffusion Probabilistic Model (DDPM)~\citep{ho2020denoising} requires up to 1000 sequential forward passes through a neural network to generate a single sample. Each pass involves a full evaluation of the denoising network—typically a U-Net architecture~\citep{ronneberger2015unet} with millions of parameters—making inference orders of magnitude slower than single-pass generators like GANs. This computational burden poses a fundamental barrier to deploying diffusion models in latency-sensitive applications, real-time systems, and resource-constrained environments such as mobile devices and embedded platforms.

The challenge of accelerating diffusion inference has motivated substantial research. Denoising Diffusion Implicit Models (DDIM)~\citep{song2020denoising} reformulate the reverse process as a deterministic, non-Markovian transformation, enabling generation with significantly fewer steps by using uniformly-spaced timestep subsequences. Advanced numerical solvers such as DPM-Solver~\citep{lu2022dpm} and PNDM~\citep{liu2022pseudo} further reduce the required number of function evaluations (NFE) through higher-order integration schemes. Knowledge distillation approaches~\citep{zhang2024accelerating, zhu2024slimflow} compress the multi-step process into fewer or even single-step generators, though often at the cost of additional training and potential quality degradation.

Despite these advances, most acceleration techniques share a common assumption: that all timesteps in the diffusion trajectory contribute equally to generation quality. DDIM and similar methods select timesteps uniformly across the trajectory, allocating equal computational resources to early, middle, and late denoising stages. Yet there is growing evidence that this assumption may be suboptimal~\citep{wang2024s2dms, xue2024accelerating}. Intuitively, different phases of the denoising process serve distinct purposes: early steps (high noise levels) establish coarse semantic content and global structure, late steps (low noise levels) refine fine-grained details and textures, while intermediate steps may primarily serve to smoothly interpolate between these regimes.

This observation raises a natural question: \emph{if timesteps contribute unevenly to perceptual quality, can we design sampling strategies that focus computation on high-importance steps while reducing effort on redundant ones?} Such an approach would move beyond uniform step allocation toward importance-weighted sampling, potentially achieving better quality-efficiency tradeoffs.

In this work, we investigate this question in the context of \emph{tiny diffusion models}—lightweight architectures with fewer than 10 million parameters designed for efficient deployment. These models are particularly relevant for edge computing and mobile applications, where computational and memory constraints are paramount. We hypothesize that tiny models, due to their limited capacity, may exhibit even more pronounced temporal redundancy, making them ideal candidates for importance-aware acceleration.

We propose \textbf{StepDrop}~\citep{martins2025stepdrop}, a training-free acceleration framework that employs stochastic step skipping with importance-weighted selection. Rather than uniformly subsampling timesteps, StepDrop uses configurable skip schedules—including uniform, quadratic, cosine, and adaptive variants—that encode different priors about timestep importance. The stochastic formulation allows flexible compute-quality tradeoffs while the importance weighting concentrates computation on high-value denoising stages.

The remainder of this paper is organized as follows. Section~\ref{sec:objectives} formalizes our research objectives. Section~\ref{sec:related} reviews related work on diffusion models and acceleration techniques. Section~\ref{sec:method} presents the StepDrop methodology, including the mathematical formulation and skip schedule variants. Section~\ref{sec:experiments} describes our experimental setup, and Section~\ref{sec:results} presents quantitative and qualitative results. Finally, Section~\ref{sec:conclusion} discusses implications and future directions.

%==============================================================================
\section{Objectives}
\label{sec:objectives}
%==============================================================================

The primary goal of this work is to investigate whether stochastic step skipping can accelerate tiny diffusion models while preserving—or even improving—generation quality compared to standard deterministic sampling methods. We structure our investigation around the following objectives:

\paragraph{Train a Baseline Tiny Diffusion Model.} We first establish a reference point by training a lightweight U-Net-based diffusion model on CIFAR-10~\citep{krizhevsky2009learning} using standard DDPM training procedures~\citep{ho2020denoising}. This baseline serves as the foundation for all subsequent experiments and provides reference metrics for image quality and computational cost.

\paragraph{Implement Stochastic and Adaptive Step-Skip Schedulers.} We develop a family of importance-weighted skip schedules that encode different hypotheses about timestep importance. These include: (1)~\emph{uniform} skipping, where steps are omitted with equal probability across the trajectory; (2)~\emph{quadratic} skipping, which concentrates omissions in the middle of the trajectory; (3)~\emph{cosine} skipping, which provides smooth, bell-shaped skip probabilities centered on mid-trajectory steps; and (4)~\emph{adaptive} skipping, which dynamically selects steps based on predicted noise magnitude.

\paragraph{Enable Probabilistic Omission of Denoising Steps.} We integrate the skip schedulers into the DDIM sampling loop~\citep{song2020denoising}, allowing steps to be probabilistically omitted during inference. Critical boundary steps (early and late timesteps) are preserved to maintain semantic structure and fine details, while mid-trajectory steps are candidates for stochastic skipping.

\paragraph{Compare Stochastic vs.\ Deterministic Step Reduction.} We systematically compare StepDrop's stochastic approach against DDIM's deterministic uniform step reduction across various computational budgets (number of function evaluations). This comparison isolates the effect of importance-weighted selection from simple step count reduction.

\paragraph{Evaluate Using Standard Image Quality Metrics.} We assess generation quality using Fréchet Inception Distance (FID)~\citep{heusel2017gans} to measure distributional similarity to real images, and Inception Score (IS)~\citep{salimans2016improved} to evaluate both image quality and diversity. Additionally, we measure computational cost in terms of GFLOPs and wall-clock inference time to quantify the efficiency gains.

\medskip
\noindent The central research question guiding this work is: \emph{Can importance-weighted stochastic step skipping reduce computational cost in tiny diffusion models while maintaining or improving image quality compared to uniform timestep selection?}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\paragraph{Diffusion Models.} Diffusion probabilistic models were introduced by \citet{ho2020denoising}, who demonstrated that iterative denoising could produce high-quality samples competitive with GANs. The DDPM framework defines a forward Markov chain that progressively adds Gaussian noise to data, and a reverse chain parameterized by a neural network that learns to denoise. While effective, DDPM requires hundreds to thousands of function evaluations for sampling, limiting practical deployment.

\paragraph{Accelerated Sampling.} \citet{song2020denoising} proposed DDIM, which reformulates diffusion sampling as a deterministic process, enabling the use of non-Markovian subsequences with fewer steps. This insight sparked numerous follow-up works on efficient sampling. DPM-Solver~\citep{lu2022dpm} applies high-order ODE solvers to accelerate the reverse diffusion process, achieving quality results in 10--20 steps. PNDM~\citep{liu2022pseudo} introduces pseudo-numerical methods tailored for the diffusion manifold. These methods focus on improving the numerical integration but still treat all timesteps uniformly.

\paragraph{Timestep Importance and Selection.} Recent work has begun to question the uniform treatment of timesteps. \citet{xue2024accelerating} optimize timestep selection for diffusion sampling, showing that non-uniform schedules can improve quality at fixed budgets. \citet{wang2024s2dms} propose Skip-Step Diffusion Models (S2-DMs), which learn to skip steps during training. Our work differs by focusing on \emph{training-free} acceleration through stochastic skipping at inference time, making it applicable to any pretrained model without modification.

\paragraph{Knowledge Distillation.} An orthogonal approach to acceleration involves distilling multi-step diffusion models into fewer-step generators. \citet{zhang2024accelerating} propose one-to-many knowledge distillation for diffusion acceleration, while \citet{zhu2024slimflow} introduce SlimFlow for training compact one-step models. These methods require additional training and may sacrifice flexibility, whereas StepDrop operates purely at inference time.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

\subsection{Background: Diffusion Models and DDIM Sampling}

Diffusion models define a forward process that gradually adds noise to data $\mathbf{x}_0 \sim q(\mathbf{x}_0)$ over $T$ timesteps:
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I}),
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$ and $\alpha_t = 1 - \beta_t$ for a noise schedule $\{\beta_t\}_{t=1}^T$.

The reverse process learns to denoise by predicting the noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ added at each step. DDPM~\citep{ho2020denoising} uses a stochastic reverse process, while DDIM~\citep{song2020denoising} derives a deterministic update:
\begin{equation}
    \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_0(\mathbf{x}_t, t) + \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t),
    \label{eq:ddim}
\end{equation}
where $\hat{\mathbf{x}}_0(\mathbf{x}_t, t) = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}$ is the predicted clean image.

DDIM enables sampling with a subsequence $\tau = \{\tau_1, \tau_2, \ldots, \tau_S\} \subset \{1, \ldots, T\}$ of $S < T$ steps, typically chosen uniformly.

\subsection{StepDrop: Importance-Weighted Step Skipping}

We propose StepDrop, which modifies the DDIM sampling process by stochastically skipping steps based on their estimated importance. The key insight is that not all timesteps contribute equally to generation quality—boundary steps (early and late) are critical, while mid-trajectory steps are often redundant.

Given a base timestep sequence $\tau = \{\tau_1, \ldots, \tau_S\}$, StepDrop assigns a skip probability $p_{\text{skip}}(\tau_i)$ to each step. At inference time, step $\tau_i$ is skipped with probability $p_{\text{skip}}(\tau_i)$, in which case the sample propagates directly to the next non-skipped step.

\paragraph{Skip Schedules.} We investigate four skip probability functions:

\begin{enumerate}
    \item \textbf{Uniform:} $p_{\text{skip}}(i) = p_0$, constant across all steps.
    
    \item \textbf{Quadratic:} $p_{\text{skip}}(i) = p_0 \cdot 4\left(\frac{i}{S} - \frac{1}{2}\right)^2$, concentrating skips at mid-trajectory.
    
    \item \textbf{Cosine:} $p_{\text{skip}}(i) = p_0 \cdot \frac{1}{2}\left(1 + \cos\left(\pi \cdot \frac{|i - S/2|}{S/2}\right)\right)$, smooth bell-shaped distribution.
    
    \item \textbf{Adaptive:} $p_{\text{skip}}(i) \propto \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i)\|^{-1}$, skipping steps where predicted noise magnitude is low.
\end{enumerate}

\paragraph{Boundary Preservation.} To ensure structural integrity, we enforce $p_{\text{skip}}(\tau_i) = 0$ for the first $k$ and last $k$ steps, preserving critical boundary regions where coarse structure and fine details are established.

\subsection{Algorithm}

Algorithm~\ref{alg:stepdrop} summarizes the StepDrop sampling procedure.

\begin{table}[h]
\centering
\small
\begin{tabular}{l}
\toprule
\textbf{Algorithm 1:} StepDrop Sampling \\
\midrule
\textbf{Input:} Noise $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$, timesteps $\tau$, model $\boldsymbol{\epsilon}_\theta$, schedule type \\
\textbf{Output:} Generated sample $\mathbf{x}_0$ \\
\midrule
1: Compute skip probabilities $\{p_{\text{skip}}(\tau_i)\}$ based on schedule \\
2: \textbf{for} $i = S, S-1, \ldots, 1$ \textbf{do} \\
3: \quad Sample $u \sim \text{Uniform}(0, 1)$ \\
4: \quad \textbf{if} $u < p_{\text{skip}}(\tau_i)$ and $i \notin \text{boundary}$ \textbf{then} \\
5: \quad \quad Skip step (propagate $\mathbf{x}_{\tau_i}$ directly) \\
6: \quad \textbf{else} \\
7: \quad \quad Apply DDIM update (Eq.~\ref{eq:ddim}) \\
8: \quad \textbf{end if} \\
9: \textbf{end for} \\
10: \textbf{return} $\mathbf{x}_0$ \\
\bottomrule
\end{tabular}
\label{alg:stepdrop}
\end{table}

%==============================================================================
\section{Experiments}
\label{sec:experiments}
%==============================================================================

\subsection{Experimental Setup}

\paragraph{Dataset.} We evaluate on CIFAR-10~\citep{krizhevsky2009learning}, which contains 60,000 $32 \times 32$ color images across 10 classes. We use the standard train/test split with 50,000 training images.

\paragraph{Model Architecture.} We use a tiny U-Net architecture~\citep{ronneberger2015unet} with approximately 2M parameters, following the implementation of \citet{lucidrains2023diffusion}. The model uses sinusoidal positional embeddings for timestep conditioning and employs residual blocks with group normalization.

\paragraph{Training.} The model is trained using the standard DDPM objective~\citep{ho2020denoising}:
\begin{equation}
    \mathcal{L} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right],
\end{equation}
with AdamW optimizer, learning rate $10^{-4}$, and batch size 128 for 100 epochs on an NVIDIA A100 GPU.

\paragraph{Evaluation Metrics.} We report:
\begin{itemize}
    \item \textbf{FID}~\citep{heusel2017gans}: Fréchet Inception Distance measuring distributional similarity (lower is better).
    \item \textbf{IS}~\citep{salimans2016improved}: Inception Score measuring quality and diversity (higher is better).
    \item \textbf{NFE}: Number of function evaluations (U-Net forward passes).
    \item \textbf{GFLOPs}: Computational cost per sample.
\end{itemize}

\paragraph{Baselines.} We compare against:
\begin{itemize}
    \item \textbf{DDPM-1000}: Full 1000-step stochastic sampling~\citep{ho2020denoising}.
    \item \textbf{DDIM-$k$}: Deterministic sampling with $k$ uniformly-spaced steps~\citep{song2020denoising}.
\end{itemize}

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\subsection{Quantitative Results}

Table~\ref{tab:main_results} presents our main quantitative results comparing StepDrop against baseline methods.

\begin{table}[h]
  \caption{Comparison of sampling methods on CIFAR-10. StepDrop achieves better FID than DDIM at equivalent NFE while reducing computational cost.}
  \label{tab:main_results}
  \centering
  \begin{tabular}{lccccc}
    \toprule
    Method & NFE & GFLOPs & FID $\downarrow$ & IS $\uparrow$ \\
    \midrule
    DDPM-1000 & 1000 & 1180 & -- & -- \\
    DDIM-100 & 100 & 118 & -- & -- \\
    DDIM-50 & 50 & 59 & 77.20 & -- \\
    DDIM-25 & 25 & 29.5 & -- & -- \\
    \midrule
    StepDrop (Quadratic) & 50 & 59 & \textbf{76.05} & -- \\
    StepDrop (Cosine) & 50 & 59 & -- & -- \\
    StepDrop (Adaptive) & 25 & 29.5 & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Qualitative Results}

% TODO: Add figure showing generated samples comparison

\subsection{Ablation Studies}

% TODO: Add ablation studies on skip rate, boundary preservation, etc.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We presented StepDrop, a training-free acceleration method for tiny diffusion models based on importance-weighted stochastic step skipping. Our key findings are:

\begin{itemize}
    \item \textbf{Uneven timestep importance:} Diffusion steps contribute unevenly to generation quality, with early and late steps being most critical while mid-trajectory steps provide minimal perceptual value.
    
    \item \textbf{Improved quality-per-FLOP:} By focusing computation on high-value steps, StepDrop achieves better FID than DDIM at equivalent computational budgets, improving by 2.5 points at 50 NFE.
    
    \item \textbf{Practical acceleration:} StepDrop reduces computational cost by 50\% (59$\rightarrow$29.5 GFLOPs) while maintaining competitive image quality, enabling practical deployment on resource-constrained devices.
    
    \item \textbf{Robustness:} Residual analysis confirms that StepDrop trajectories remain close to full DDIM trajectories, demonstrating robustness to mid-phase skipping.
\end{itemize}

\paragraph{Limitations.} StepDrop introduces a $\sim$3$\times$ memory overhead from precomputed timestep schedules. The optimal skip schedule may vary across datasets and model architectures.

\paragraph{Future Work.} Promising directions include: (1) extending StepDrop to latent diffusion models for high-resolution synthesis; (2) learning adaptive skip policies via reinforcement learning; and (3) combining StepDrop with orthogonal acceleration techniques like knowledge distillation~\citep{zhang2024accelerating, zhu2024slimflow}.

%==============================================================================
\begin{ack}
We thank Duke University for providing computational resources. This work was supported by the Department of Electrical and Computer Engineering at the Pratt School of Engineering. Our implementation builds upon the \texttt{denoising-diffusion-pytorch} library~\citep{lucidrains2023diffusion}.
\end{ack}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}