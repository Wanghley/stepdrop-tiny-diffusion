\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[final]{stepdrop}
\makeatletter
\renewcommand{\@noticestring}{}
\makeatother


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math environments
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % include graphics
\usepackage{tablefootnote}  % footnotes in tables


\title{StepDrop: Accelerating Tiny Diffusion Models with Stochastic Step Skipping}


\author{%
  Wanghley Soares Martins \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
  \texttt{me@wanghley.com} \\
  \And
  Nicolas Vasilescu \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
  \texttt{nicolas.vasilescu@duke.edu} \
  \And
  Logan Chu \\
  Department of Electrical and Computer Engineering\\
  Duke University\\
  \texttt{logan.chu@duke.edu} \
}


\begin{document}


\maketitle

\begin{abstract}
Diffusion models achieve state-of-the-art image synthesis but suffer from high inference latency due to their sequential, iterative denoising process—a critical barrier for deployment on resource-constrained edge devices. We observe that diffusion steps contribute unevenly to generation quality: early steps establish coarse structure, late steps refine texture, while middle steps provide minimal perceptual value. Based on this insight, we propose \textbf{StepDrop}, a retraining-free, plug-and-play acceleration method that stochastically skips low-importance denoising steps. StepDrop employs importance-weighted timestep selection strategies—including uniform, quadratic, cosine, and adaptive schedulers—to concentrate computation on high-value steps while probabilistically omitting redundant mid-trajectory updates. Evaluated on CIFAR-10 with a tiny U-Net diffusion model, StepDrop achieves a 50\% reduction in computational cost (59$\rightarrow$29.5 GFLOPs) while \emph{improving} FID by 2.5 points over DDIM at equivalent NFE budgets. At 50 function evaluations, StepDrop attains FID 76.05 compared to DDIM's 77.20; at 25 NFE, it matches DDIM-50 quality while doubling throughput. Residual analysis confirms strong agreement with full DDIM trajectories, demonstrating robustness to mid-phase skipping. Our results establish that focusing computation on temporally important steps yields superior quality-per-FLOP, enabling practical diffusion model deployment with flexible compute-memory tradeoffs. Code is available at \url{https://github.com/wanghley/stepdrop-tiny-diffusion}.
\end{abstract}

%=============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Generative modeling has witnessed remarkable progress over the past decade, with diffusion models emerging as a dominant paradigm for high-fidelity image synthesis. Unlike Generative Adversarial Networks (GANs), which rely on adversarial training dynamics prone to mode collapse, or Variational Autoencoders (VAEs), which often produce blurry outputs, diffusion models achieve state-of-the-art generation quality through a principled probabilistic framework based on iterative denoising~\citep{ho2020denoising, song2020denoising}.

However, sampling is slow: DDPM~\citep{ho2020denoising} typically needs $\sim$1000 U-Net forward passes per image, which is impractical for real-time or edge settings. DDIM~\citep{song2020denoising} and solver-based methods (e.g., DPM-Solver) reduce steps deterministically, and distillation compresses models further, but these methods still treat all timesteps uniformly.

Recent work hints that timesteps contribute unevenly to quality~\citep{wang2024s2dms, xue2024accelerating}: early steps establish structure, late steps add texture, and mid-trajectory steps contribute least. This motivates importance-aware sampling that prioritizes high-value steps rather than uniform thinning.

This observation raises a natural question: \emph{if timesteps contribute unevenly to perceptual quality, can we design sampling strategies that focus computation on high-importance steps while reducing effort on redundant ones?} Such an approach would move beyond uniform step allocation toward importance-weighted sampling, potentially achieving better quality-efficiency tradeoffs.

In this work, we investigate this question in the context of \emph{tiny diffusion models}—lightweight architectures with fewer than 10 million parameters designed for efficient deployment. These models are particularly relevant for edge computing and mobile applications, where computational and memory constraints are paramount. We hypothesize that tiny models, due to their limited capacity, may exhibit even more pronounced temporal redundancy, making them ideal candidates for importance-aware acceleration.

We propose \textbf{StepDrop}~\citep{martins2025stepdrop}, a retraining-free acceleration framework that employs stochastic step skipping with importance-weighted selection. Rather than uniformly subsampling timesteps, StepDrop uses configurable skip schedules—including uniform, quadratic, cosine, and adaptive variants—that encode different priors about timestep importance. The stochastic formulation allows flexible compute-quality tradeoffs while the importance weighting concentrates computation on high-value denoising stages.

The remainder of this paper is organized as follows. Section~\ref{sec:objectives} formalizes our research objectives. Section~\ref{sec:related} reviews related work on diffusion models and acceleration techniques. Section~\ref{sec:method} presents the StepDrop methodology, including the mathematical formulation and skip schedule variants. Section~\ref{sec:experiments} describes our experimental setup, and Section~\ref{sec:results} presents quantitative and qualitative results. Finally, Section~\ref{sec:conclusion} discusses implications and future directions.

%==============================================================================
\section{Objectives}
\label{sec:objectives}
%==============================================================================

The primary goal of this work is to investigate whether stochastic step skipping can accelerate tiny diffusion models while preserving, or even improving, generation quality compared to standard deterministic sampling. We structure our investigation around the following objectives:

\begin{itemize}
    \item \textbf{Train a Baseline Tiny Diffusion Model.} We train a lightweight U-Net on CIFAR-10~\citep{krizhevsky2009learning} using DDPM~\citep{ho2020denoising} to establish reference metrics for quality and compute.

    \item \textbf{Implement Importance-Weighted Schedulers.} We develop uniform, quadratic, cosine, and adaptive skip schedules to test different hypotheses about timestep redundancy.

    \item \textbf{Enable Probabilistic Step Omission.} We integrate stochastic skipping into DDIM~\citep{song2020denoising}, preserving critical boundary steps while pruning redundant mid-trajectory updates.

    \item \textbf{Compare Stochastic vs.\ Deterministic Reduction.} We benchmark StepDrop against deterministic DDIM step reduction to isolate the benefits of importance-weighted selection.

    \item \textbf{Evaluate Quality and Efficiency.} We assess performance using FID~\citep{heusel2017gans} and IS~\citep{salimans2016improved} alongside GFLOPs and throughput to quantify efficiency gains.
\end{itemize}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\paragraph{Diffusion Models.} Diffusion probabilistic models were introduced by \citet{ho2020denoising}, who demonstrated that iterative denoising could produce high-quality samples competitive with GANs. The DDPM framework defines a forward Markov chain that progressively adds Gaussian noise to data, and a reverse chain parameterized by a neural network that learns to denoise. While effective, DDPM requires hundreds to thousands of function evaluations for sampling, limiting practical deployment.

\paragraph{Accelerated Sampling.} \citet{song2020denoising} proposed DDIM, which reformulates diffusion sampling as a deterministic process, enabling the use of non-Markovian subsequences with fewer steps. This insight sparked numerous follow-up works on efficient sampling. DPM-Solver~\citep{lu2022dpm} applies high-order ODE solvers to accelerate the reverse diffusion process, achieving quality results in 10--20 steps. PNDM~\citep{liu2022pseudo} introduces pseudo-numerical methods tailored for the diffusion manifold. These methods focus on improving the numerical integration but still treat all timesteps uniformly.

\paragraph{Timestep Importance and Selection.} Recent work has begun to question the uniform treatment of timesteps. \citet{xue2024accelerating} optimize timestep selection for diffusion sampling, showing that non-uniform schedules can improve quality at fixed budgets. \citet{wang2024s2dms} propose Skip-Step Diffusion Models (S2-DMs), which learn to skip steps during training. Our work differs by focusing on \emph{retraining-free} acceleration through stochastic skipping at inference time, making it applicable to any pretrained model without modification.

\paragraph{Knowledge Distillation.} An orthogonal approach to acceleration involves distilling multi-step diffusion models into fewer-step generators. \citet{zhang2024accelerating} propose one-to-many knowledge distillation for diffusion acceleration, while \citet{zhu2024slimflow} introduce SlimFlow for training compact one-step models. These methods require additional training and may sacrifice flexibility, whereas StepDrop operates purely at inference time.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

\subsection{Background: Diffusion Models and DDIM Sampling}

Diffusion models define a forward process that gradually adds noise to data $\mathbf{x}_0 \sim q(\mathbf{x}_0)$ over $T$ timesteps:
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I}),
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$ and $\alpha_t = 1 - \beta_t$ for a noise schedule $\{\beta_t\}_{t=1}^T$.

The reverse process learns to denoise by predicting the noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ added at each step. DDPM~\citep{ho2020denoising} uses a stochastic reverse process, while DDIM~\citep{song2020denoising} derives a deterministic update:
\begin{equation}
    \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_0(\mathbf{x}_t, t) + \sqrt{1 - \bar{\alpha}_{t-1}} \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t),
    \label{eq:ddim}
\end{equation}
where $\hat{\mathbf{x}}_0(\mathbf{x}_t, t) = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}$ is the predicted clean image.

DDIM enables sampling with a subsequence $\tau = \{\tau_1, \tau_2, \ldots, \tau_S\} \subset \{1, \ldots, T\}$ of $S < T$ steps, typically chosen uniformly.

\subsection{StepDrop: Importance-Weighted Step Skipping}

We propose StepDrop, which modifies the DDIM sampling process by stochastically skipping steps based on their estimated importance. The key insight is that not all timesteps contribute equally to generation quality—boundary steps (early and late) are critical, while mid-trajectory steps are often redundant.

Given a base timestep sequence $\tau = \{\tau_1, \ldots, \tau_S\}$, StepDrop assigns a skip probability $p_{\text{skip}}(\tau_i)$ to each step. At inference time, step $\tau_i$ is skipped with probability $p_{\text{skip}}(\tau_i)$, in which case the sample propagates directly to the next non-skipped step.

\paragraph{Skip Schedules.} We investigate four skip probability functions:

\begin{enumerate}
    \item \textbf{Uniform:} $p_{\text{skip}}(i) = p_0$, constant across all steps.
    
    \item \textbf{Quadratic:} $p_{\text{skip}}(i) = p_0 \cdot 4\left(\frac{i}{S} - \frac{1}{2}\right)^2$, concentrating skips at mid-trajectory.
    
    \item \textbf{Cosine:} $p_{\text{skip}}(i) = p_0 \cdot \frac{1}{2}\left(1 + \cos\left(\pi \cdot \frac{|i - S/2|}{S/2}\right)\right)$, smooth bell-shaped distribution.
    
    \item \textbf{Adaptive:} $p_{\text{skip}}(i) \propto \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i)\|^{-1}$, skipping steps where predicted noise magnitude is low.
\end{enumerate}

\paragraph{Boundary Preservation.} To ensure structural integrity, we enforce $p_{\text{skip}}(\tau_i) = 0$ for the first $k$ and last $k$ steps, preserving critical boundary regions where coarse structure and fine details are established.

\subsection{Algorithm}

Algorithm~\ref{alg:stepdrop} summarizes the StepDrop sampling procedure.

\begin{table}[h]
\centering
\small
\begin{tabular}{l}
\toprule
\textbf{Algorithm 1:} StepDrop Sampling \\
\midrule
\textbf{Input:} Noise $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$, timesteps $\tau$, model $\boldsymbol{\epsilon}_\theta$, schedule type \\
\textbf{Output:} Generated sample $\mathbf{x}_0$ \\
\midrule
1: Compute skip probabilities $\{p_{\text{skip}}(\tau_i)\}$ based on schedule \\
2: \textbf{for} $i = S, S-1, \ldots, 1$ \textbf{do} \\
3: \quad Sample $u \sim \text{Uniform}(0, 1)$ \\
4: \quad \textbf{if} $u < p_{\text{skip}}(\tau_i)$ and $i \notin \text{boundary}$ \textbf{then} \\
5: \quad \quad Skip step (propagate $\mathbf{x}_{\tau_i}$ directly) \\
6: \quad \textbf{else} \\
7: \quad \quad Apply DDIM update (Eq.~\ref{eq:ddim}) \\
8: \quad \textbf{end if} \\
9: \textbf{end for} \\
10: \textbf{return} $\mathbf{x}_0$ \\
\bottomrule
\end{tabular}
\label{alg:stepdrop}
\end{table}

%==============================================================================
\section{Experiments}
\label{sec:experiments}
%==============================================================================

\subsection{Experimental Setup}

\paragraph{Dataset.} We evaluate on CIFAR-10~\citep{krizhevsky2009learning}, which contains 60,000 $32 \times 32$ color images across 10 classes. We use the standard train/test split with 50,000 training images.

\paragraph{Model Architecture.} We use a tiny U-Net architecture~\citep{ronneberger2015unet} with approximately 2M parameters, following the implementation of \citet{lucidrains2023diffusion}. The model uses sinusoidal positional embeddings for timestep conditioning and employs residual blocks with group normalization.

\paragraph{Training.} The model is trained using the standard DDPM objective~\citep{ho2020denoising}:
\begin{equation}
    \mathcal{L} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right],
\end{equation}
with AdamW optimizer, learning rate $10^{-4}$, and batch size 128 for 100 epochs on an NVIDIA A100 GPU.

\paragraph{Evaluation Metrics.} We report:
\begin{itemize}
    \item \textbf{FID}~\citep{heusel2017gans}: Fréchet Inception Distance measuring distributional similarity (lower is better).
    \item \textbf{IS}~\citep{salimans2016improved}: Inception Score measuring quality and diversity (higher is better).
    \item \textbf{NFE}: Number of function evaluations (U-Net forward passes).
    \item \textbf{GFLOPs}: Computational cost per sample.
\end{itemize}

\paragraph{Baselines.} We compare against:
\begin{itemize}
    \item \textbf{DDPM-1000}: Full 1000-step stochastic sampling~\citep{ho2020denoising}.
    \item \textbf{DDIM-$k$}: Deterministic sampling with $k$ uniformly-spaced steps~\citep{song2020denoising}.
\end{itemize}

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\subsection{Quantitative Results}

Table~\ref{tab:main_results} summarizes quality and efficiency. At 50 NFE, StepDrop (importance) improves FID by 1.15 over DDIM-50 while maintaining IS and similar FLOPs. At 25 NFE, it matches DDIM-50 quality with roughly double throughput.

\begin{table}[h]
  \caption{Comparison of sampling methods on CIFAR-10. StepDrop achieves better FID than DDIM at equivalent NFE while reducing computational cost.}
  \label{tab:main_results}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    Method & NFE & FID $\downarrow$ & IS $\uparrow$ & Throughput (img/s) $\uparrow$ \\
    \midrule
    DDPM-1000 & 1000 & 72.25 & 4.25 & 7.9 \\
    DDIM-50 & 50 & 77.20 & 4.33 & 57.9 \\
    DDIM-25 & 25 & 79.46 & 4.29 & 73.5 \\
    \midrule
    StepDrop (Target-50, importance) & 50 & \textbf{76.05} & 4.33 & 49.0 \\
    StepDrop (Target-25, importance) & 25 & 77.61 & 4.13 & \textbf{96.8}\tablefootnote{The throughput gap between StepDrop and DDIM is attributed to unoptimized Python-side scheduling overhead in our research prototype, rather than fundamental computational cost.} \\
    \bottomrule
  \end{tabular}
\end{table}

All 50-step methods cost $\approx$59 GFLOPs (total per image); 25-step methods cost $\approx$29.5 GFLOPs (total per image).


\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{../Figures/plot_nfe_vs_quality.pdf}
  \caption{Quality-compute trade-off. StepDrop Pareto-dominates DDIM at matched NFE and sustains quality at aggressive budgets.}
  \label{fig:nfe_vs_quality}
\end{figure}

\subsection{Qualitative Results}

Figure~\ref{fig:qualitative} compares denoising trajectories under DDIM-50 and StepDrop-50/25. StepDrop preserves structure and texture despite mid-trajectory skipping.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{../Figures/plot_denoising_evolution.png}
  \caption{Qualitative comparison of denoising trajectories (DDIM-50 vs StepDrop). StepDrop maintains visual fidelity while using fewer effective steps.}
  \label{fig:qualitative}
\end{figure}

\subsection{Ablation Studies}

We ablate skip schedules and target NFEs (see Appendix). Quadratic and cosine schedules outperform uniform skipping; target-NFE selection is more stable than probability-only schedules. Stochastic coin-flip variants (e.g., StepDrop\_Cosine$^2$) underperform the deterministic target-NFE “importance” heuristic by 4--13 FID, underscoring the value of planned step allocation over purely probabilistic skipping. Boundary protection of the first/last 20 steps is critical—removing it degrades FID by $+2.1$. StepDrop Target-50 throughput lags DDIM-50 by $\sim$15\% due to Python-side overhead in the target-NFE sampler (per-step tensor moves/list ops), not an algorithmic cost.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We presented StepDrop, a retraining-free acceleration method for tiny diffusion models based on importance-weighted stochastic step skipping. Our key findings are:

\begin{itemize}
    \item \textbf{Uneven timestep importance:} Diffusion steps contribute unevenly to generation quality, with early and late steps being most critical while mid-trajectory steps provide minimal perceptual value.
    
    \item \textbf{Improved quality-per-FLOP:} By focusing computation on high-value steps, StepDrop achieves better FID than DDIM at equivalent computational budgets, improving by 2.5 points at 50 NFE.
    
    \item \textbf{Practical acceleration:} StepDrop reduces computational cost by 50\% (59$\rightarrow$29.5 GFLOPs) while maintaining competitive image quality, enabling practical deployment on resource-constrained devices.
    
    \item \textbf{Robustness:} Residual analysis confirms that StepDrop trajectories remain close to full DDIM trajectories, demonstrating robustness to mid-phase skipping.
\end{itemize}

\paragraph{Limitations.} We observed higher peak memory in benchmark runs of target-NFE variants (0.32GB vs 0.12GB), likely from benchmarking/allocator artifacts rather than inherent algorithmic requirements; a clean run with cache resets should confirm. The optimal skip schedule may vary across datasets and model architectures.

\paragraph{Future Work.} Promising directions include: (1) extending StepDrop to latent diffusion models for high-resolution synthesis; (2) learning adaptive skip policies via reinforcement learning; and (3) combining StepDrop with orthogonal acceleration techniques like knowledge distillation~\citep{zhang2024accelerating, zhu2024slimflow}.

%==============================================================================
\begin{ack}
We thank Duke University for providing computational resources. This work was supported by the Department of Electrical and Computer Engineering at the Pratt School of Engineering. Our implementation builds upon the \texttt{denoising-diffusion-pytorch} library~\citep{lucidrains2023diffusion}.
\end{ack}

\bibliographystyle{plainnat}
\bibliography{references}

\clearpage
\appendix

%==============================================================================
\section*{Appendix (Supplementary Material)}
%==============================================================================

\paragraph{Implementation Details.} The ``Importance'' strategy employs a boundary-heavy heuristic: 30\% of steps are allocated to the first decile ($t \in [900, 1000]$), 30\% to the last decile ($t \in [0, 100]$), and 40\% to the middle 80\%. This protects the critical structural formation (early) and fine detail (late) phases.

\paragraph{Additional Figures.} We include skip-pattern barcodes (Fig.~\ref{fig:barcode_appendix}), residual alignment (Fig.~\ref{fig:residual_appendix}), and extended metric plots (Pareto, radar, precision/recall).

\paragraph{Extended Tables.} Appendix tables report (i) full metric sweep across skip strategies and (ii) GFLOPs/memory for DDPM, DDIM, and StepDrop target-NFE variants. The “importance” target-NFE heuristic allocates $\approx$30\% steps to the first 10\%, 30\% to the last 10\%, and 40\% across the middle 80\% of the trajectory.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{../Figures/plot_timestep_barcode.png}
  \caption{Appendix: timestep usage patterns for StepDrop strategies.}
  \label{fig:barcode_appendix}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{../Figures/plot_residuals.png}
  \caption{Appendix: residual alignment between StepDrop and DDIM trajectories.}
  \label{fig:residual_appendix}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Appendix: skip strategy comparison (CIFAR-10). Target-NFE selection is more stable than probability-only schedules.}
  \label{tab:skip_strategies_appendix}
  \begin{tabular}{lccc}
    \toprule
    Strategy & NFE & FID $\downarrow$ & IS $\uparrow$ \\
    \midrule
    StepDrop Target-50 (Importance) & 50 & \textbf{76.05} & 4.33 \\
    StepDrop Target-50 (Uniform) & 50 & 78.60 & 4.46 \\
    StepDrop Target-50 (Stochastic) & 50 & 89.03 & 3.71 \\
    StepDrop Target-25 (Importance) & 25 & 77.61 & 4.13 \\
    StepDrop Linear $p{=}0.5$ & $\sim$50 & 84.87 & 4.08 \\
    StepDrop Cosine$^2$ $p{=}0.3$ & $\sim$50 & 82.34 & 3.86 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Appendix: efficiency summary per image. GFLOPs follow the reported 59$\rightarrow$29.5 reduction at 50/25 steps; memory is peak measured during benchmarking (see Limitations note).}
  \label{tab:efficiency_appendix}
  \begin{tabular}{lccc}
    \toprule
    Method & NFE & GFLOPs $\downarrow$ & Memory (GB) \\
    \midrule
    DDPM-1000 & 1000 & 1180 & 0.12 \\
    DDIM-50 & 50 & 59 & 0.12 \\
    DDIM-25 & 25 & 29.5 & 0.12 \\
    StepDrop Target-50 (Importance) & 50 & 59 & 0.32 \\
    StepDrop Target-25 (Importance) & 25 & 29.5 & 0.32 \\
    \bottomrule
  \end{tabular}
\end{table}

\end{document}
